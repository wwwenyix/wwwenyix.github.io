---
title: "Simon Task - Data exploration"
author: "Michael Franke"
format: 
  html:
    code-fold: true
    self-contained: true
    highlight-style: zenburn
execute:
  error: false
  warning: false
  message: false
  cache: true
editor:
  markdown:
    wrap: sentence
---

```{r}
pckgs_needed <- c(
  "tidyverse",
  "brms",
  "remotes",
  "tidybayes"
)
x <- lapply(pckgs_needed, library, character.only = TRUE)
library(aida)
library(faintr)
library(cspplot)
```


```{r loads-preps}
#| echo: true
#| error: false
#| warning: false
#| message: false

# install packages from CRAN (unless installed)
pckgs_needed <- c(
  "tidyverse",
  "brms",
  "remotes",
  "tidybayes"
)
pckgs_installed <- installed.packages()[,"Package"]
pckgs_2_install <- pckgs_needed[!(pckgs_needed %in% pckgs_installed)]
if(length(pckgs_2_install)) {
  install.packages(pckgs_2_install)
} 

# install additional packages from GitHub (unless installed)
if (! "aida" %in% pckgs_installed) {
  remotes::install_github("michael-franke/aida-package")
}
if (! "faintr" %in% pckgs_installed) {
  remotes::install_github("michael-franke/faintr")
}
if (! "cspplot" %in% pckgs_installed) {
  remotes::install_github("CogSciPrag/cspplot")
}

# load the required packages
x <- lapply(pckgs_needed, library, character.only = TRUE)
library(aida)
library(faintr)
library(cspplot)

# these options help Stan run faster
options(mc.cores = parallel::detectCores())

# use the CSP-theme for plotting
theme_set(theme_csp())

# global color scheme from CSP
project_colors = cspplot::list_colors() |> pull(hex)
# names(project_colors) <- cspplot::list_colors() |> pull(name)

# setting theme colors globally
scale_colour_discrete <- function(...) {
  scale_colour_manual(..., values = project_colors)
}
scale_fill_discrete <- function(...) {
   scale_fill_manual(..., values = project_colors)
}

```

```{r, echo = FALSE}
d <- read_csv("03_Simon_data_anonym.csv")
```

# Background: the Simon task

The Simon task is pretty cool.
The task is designed to see if responses are faster and/or more accurate when the stimulus to respond to occurs in the same relative location as the response, even if the stimulus location is irrelevant to the task.
For example, it is faster to respond to a stimulus presented on the left of the screen with a key that is on the left of the keyboard (e.g. `q`), than with a key that is on the right of the keyboard (e.g. `p`).

# Experiment

[\[REMARK: This is roughly how an experiment would also be described in a research paper. Try to match it onto the experiment as you experienced it when you performed it.\]]{style="color:firebrick"}

## Participants

```{r}
#| echo: false

N_total <- d$submission_id |> unique() |> length()
N_intro <- filter(d, class == "Intro Cogn. Neuro-Psychology") |> pull(submission_id) |> unique() |> length()
N_repos <- filter(d, class == "Experimental Psych Lab") |> pull(submission_id) |> unique() |> length()
N_both  <- filter(d, class == "both") |> pull(submission_id) |> unique() |> length()
```

A total of `r N_total` participants took part in an online version of a Simon task.
Participants were students enrolled in either "Introduction to Cognitive (Neuro-)Psychology" (N = `r N_intro`), or "Experimental Psychology Lab Practice" (N = `r N_repos`) or both (N = `r N_both`).

## Materials & Design

Each trial started by showing a fixation cross for 200 ms in the center of the screen.
Then, one of two geometrical shapes was shown for 500 ms. The **target shape** was either a blue square or a blue circle.
The target shape appeared either on the left or right of the screen.
Each trial determined uniformly at random which shape (square or circle) to show as target and where on the screen to display it (left or right).
Participants where instructed to press keys `q` (left of keyboard) or `p` (right of keyboard) to identify the kind of shape on the screen.
The shape-key allocation happened experiment initially, uniformly at random once for each participant and remained constant throughout the experiment.
For example, a participant may have been asked to press `q` for square and `p` for circle.

Trials were categorized as either 'congruent' or 'incongruent'.
They were congruent if the location of the stimulus was the same relative location as the response key (e.g. square on the right of the screen, and `p` key to be pressed for square) and incongruent if the stimulus was not in the same relative location as the response key (e.g. square on the right and `q` key to be pressed for square).

In each trial, if no key was pressed within 3 seconds after the appearance of the target shape, a message to please respond faster was displayed on screen.

## Procedure

Participants were first welcomed and made familiar with the experiment.
They were told to optimize both speed and accuracy.
They then practiced the task for 20 trials before starting the main task, which consisted of 100 trials.
Finally, the experiment ended with a post-test survey in which participants were asked for their student IDs and the class they were enrolled in.
They were also able to leave any optional comments.

# Data, factors & hypotheses

[\[REMARK: This section would not normally be part of a research paper. Here, we are trying to see more clearly that there are usually different ways of mapping the (raw) data onto categories or factors for analysis.\]]{style="color:firebrick"}

## Data from a single trial

Data from a single (main) trial of our Simon Task consists, among other things, of the following relevant pieces of information:

-   **SHAPE**: What is the shape shown on the screen? (circle vs. square)
-   **POSITION**: What is the position of the stimulus on the screen? (left vs. right)
-   **KEYMAP**: Which letter is used for 'circle'? (circle = p vs circle = q)
-   **RESPONSE**: Which letter was pressed? (p vs q)
-   **RT**: What was the reaction time, i.e., the time passed from stimulus onset to button press? (measured in milliseconds)

## Factors and factorial design: a first naive approach

Think about this for a moment.
Which of these variables are important for answering our research question?
Does it take longer to respond when the location of the stimulus and the correct button coincide?
What kind of design is this?
Factorial?
Which factors?

We could say that SHAPE and POSITION are unordered factors with two levels each and that these are manipulated within-participants.
The factor KEYMAP has two unordered levels as well, but is manipulated between-subjects.
We could therefore say that we have a $2 \times 2 \times 2$ mixed factorial design.
The dependent variable is reaction times and we have multiple measurements for each design cell.

## A more useful conceptualization

We can also conceptualize the data from a single trial in terms of a derived factor **CONGRUENCY**, unordered with two levels: the trial is *congruent* if the shape appears where the corresponding key appears, else it is *incongruent*.
The four factors SHAPE, KEYMAP, POSITION, and CONGRUENCY are mutually dependent.
We only ever need three of them, and the third would be derivable.
How should we then think of our experiment and data?

Well, that depends on the research question.
One plausible way is to drop the between-subject factor KEYMAP.
We could argue that once participants have internalized the mapping of shape to key, the precise mapping does not matter so much.
(That's an assumption! It seems plausible, but might have to be tested!)

We could then analyze the data from this experiment in terms of the tree factors:

-   **SHAPE**: What is the shape shown on the screen? (circle vs. square)
-   **POSITION**: What is the position of the stimulus on the screen? (left vs. right)
-   **CONGRUENCY**: Does the shape appear on the same side as the key that is assigned to it? (congruent vs. incongruent)

The dependent variable is RT (reaction time), and we will only look at the successful trials, i.e., those for which the RESPONSE given matches the shape that was shown.
(The latter is, strictly speaking, also a "derived variable" if we at first only assume the first list of factors given above.)

The benefit of this conceptualization is that we can directly address our research question.
In exp-psych jargon, we'd say that we expect an effect of factor CONGRUENCY.
But let's slow down and look at this more carefully.

## adding ...
```{r}
glimpse(d)
```


## Hypotheses

Based on data from correct responses, we are going to analyse the dependent variable RT in terms of the factor CONGRUENCY.
One common way for writing this concisely (as an R formula) is:

```{r eval = F}
RT ~ CONGRUENCY
```

Our research hypothesis is that (correct) responses from congruent trials have lower RT than those from incongruent trials.

We could also hold additional hypotheses about the *absence* of an effect of SHAPE and POSITION.
After all, it is hard to see how these factors should bias RTs in a systematic way.
We could, and maybe should, however, include these "nuisance factors" into the analysis as well.
We would then "control for" any potential effect that these factors might have.
A more encompassing analysis (= a more open mind regarding potential sources of systematic variation and/or noise in the data) is usually preferred.

We could then analyze the dependent variable RT (where we still only look at trials with correct responses) in terms of the factors CONGRUENCY, SHAPE and POSITION (and their interaction (more on this later when we actually look at statistics in week 5)).
One way of writing this is via the R formula (which is also more generally used notation):

```{r eval = F}
RT ~ CONGRUENCY * SHAPE * POSITION
```

We would then still expect "a main effect of CONGRUENCY" (see later for what that means) and no other significance effects, i.e., no main effects of SHAPE and POSITION and not interactions.

# Results

[\[REMARK: A research paper would include a "results" section with content similar to this one, but this one is more elaborate. The content of this section is what you would do for yourself or your team to get an overview of the data in an exploratory analysis (= one where the key decisions for cleaning and pre-processing are not preregistered). Also, a research paper would normally *not* contain any code snippets. (Open scientists would argue that at least an archive should make all the code and data freely available, so the code doesn't need to go into the main paper.)\]]{style="color:firebrick"}

## Loading and inspecting the data

We load the data into R and show a summary of the variables stored in the tibble:

```{r}
d <- read_csv("03_Simon_data_anonym.csv")
glimpse(d)
```

It is often useful to check general properties, such as the mean time participants spent on the experiment:

```{r}
d |> pull(timeSpent) |> mean()
```

About `r d |> pull(timeSpent) |> mean() |> round(2)` minutes is quite long, but we know that the mean is very susceptible to outliers, so we may want to look at a more informative set of **summary statistics**:

```{r}
d |> pull(timeSpent) |> summary()
```

We can also plot the distribution of `timeSpent`, seeing even more clearly how we had a few severe outliers:

```{r}
d |> 
  ggplot(aes(x = timeSpent)) +
  geom_histogram()
```

## Summarizing & cleaning the data

We look at outlier-y behavior at the level of individual participants first, then at the level of individual trials.

### Individual-level error rates & reaction times

It is conceivable that some participants did not take the task seriously.
They may have just fooled around.
We will therefore inspect each individual's response patterns and reaction times.
If participants appear to have "misbehaved" we discard all of their data.
(**CAVEAT:** Notice the researcher degrees of freedom in the decision of what counts as "misbehavior"! It is therefore that choices like these are best committed to in advance, e.g. via pre-registration!)

We can calculate the mean reaction times and the error rates for each participant.

```{r}
d_individual_summary <- d |> 
  filter(trial_type == "main") |>    # look at only data from main trials
  group_by(submission_id) |>         # calculate the following for each individual
  summarize(mean_RT = mean(RT),
            error_rate = mean(correctness == "incorrect"))
head(d_individual_summary)
```

Let's plot this summary information:

```{r}
d_individual_summary |> 
  ggplot(aes(x = mean_RT, y = error_rate)) +
  geom_point()
```

Here's a crude way of branding outlier-participants:

```{r}
d_individual_summary <- d_individual_summary |> 
  mutate(outlier = case_when(mean_RT < 350 ~ TRUE,
                             mean_RT > 750 ~ TRUE,
                             error_rate > 0.5 ~ TRUE,
                             TRUE ~ FALSE))
d_individual_summary |> 
  ggplot(aes(x = mean_RT, y = error_rate)) +
  geom_point() +
  geom_point(data = filter(d_individual_summary, outlier == TRUE),
             color = "firebrick", shape = "square", size = 5)

```

We then clean the data set in a first step by removing all participants identified as outlier-y:

```{r, message=TRUE}
d <- full_join(d, d_individual_summary, by = "submission_id") # merge the tibbles
d <- filter(d, outlier == FALSE)
message(
  "We excluded ", sum(d_individual_summary$outlier) , 
  " participants for suspicious mean RTs and higher error rates."
)
```

### Trial-level reaction times

It is also conceivable that individual trials resulted in early accidental key presses or were interrupted in some way or another.
We therefore look at the overall distribution of RTs and determine (similarly arbitrarily, but once again this should be planned in advance) what to exclude.

```{r}
d |> ggplot(aes(x = log(RT))) +
  geom_histogram() +
  geom_jitter(aes(x = log(RT), y = 1), alpha = 0.3, height = 300)
```

Let's decide to exclude all trials that lasted longer than 1 second and also all trials with reaction times under 100 ms.

```{r}
d <- filter(d, RT > 100 & RT < 1000)
d |> ggplot(aes(x = RT)) +
  geom_histogram() +
  geom_jitter(aes(x = RT, y = 1), alpha = 0.3, height = 300)
```

## Exploring the (main) data

### Means for relevant conditions

As explained in the previous section, we are interested mainly in an effect of CONGRUENCY on the average RT for trials that resulted in correct responses.
Here is a summary of the means and standard deviations for each level of the factor CONGRUENCY (which is called `condition` in the data set at hand):

```{r}
d_sum <- d |> 
  # look at only correct trials
  filter(correctness == "correct") |> 
  # look at each level of factor 'condition'
  group_by(condition) |> 
  # get the mean and SD or the reaction time
  summarize(mean_RT = mean(RT),
            sd_RT = sd(RT))
d_sum
```

The mean RT of the congruent trials is lower than that of the incongruent trials.
But that numeric comparison is not enough.
We should also visualize the data and employ statistical inference.

### Plotting the reaction times

Here's a plot of the reaction times split up by factor CONGRUENCY

```{r}
d |> 
  # look at only correct trials
  filter(correctness == "correct") |> 
  # make a basic plot and declare axes
  ggplot(aes(x = RT)) +
  # add individual data point (jittered)
  geom_jitter(aes(y = 0.0005), alpha = 0.1, height = 0.0005) +
  # add a density plot
  geom_density(fill = "gray", alpha = 0.5) +
  # add a vertical line indicating the mean
  geom_vline(data = d_sum, 
             mapping = aes(xintercept = mean_RT), 
             color = "firebrick") +
  # producing facets for each CONGRUENCY level
  facet_grid(condition ~ .)
```

# Data analysis

[\[REMARK: This section is mainly written like you would do it in a research paper, a thesis or a term paper. But notice that, ideally, exactly what we do here and the criteria by which we test our hypotheses of interest should be pre-registered *before* the data are known.\]]{style="color:firebrick"}

Remember that our main research hypothesis is that reaction times are higher for incongruent trials than for congruent trials.
We also entertain the possibility of effects from POSITION (whether the object appeared left or right on the screen) and SHAPE (whether the target object was a circle or a square).
We test this with a Bayesian regression model regressing RT on CONGRUENCY, SHAPE and POSITION and their interactions using the the R package `brms`.
We use the `faintr` package to test the following directed research hypothesis:

-   main effect of CONGRUENCY on RT, such that incongruent trials take longer

We also address, out of curiosity, the following adjunct hypotheses:

-   no main effect of SHAPE
-   no main effect of POSITION

We test a directed hypothesis by inspecting the probability that the coefficient in question is bigger or smaller than zero.
We test an undirected hypothesis by checking whether 0 is contained in the the 95% HDI.

We first run the model.

```{r, results = 'hide', error=FALSE}
ST_model <- brm(
  formula = RT ~ condition * target_object * target_position,
  data = d |> filter(correctness == "correct")  
)

```

We then use the `faintr` package to check whether the estimated mean average over all cells with incongruent trials is higher than that for cells with congruent data.

```{r}
faintr::compare_groups(
  fit = ST_model, 
  higher = condition == "incongruent", 
  lower = condition  == "congruent"
)
```

We find that the posterior probability that this is so, given model and data, is almost certain.

Next, we test for a main effect of factor POSITION:

```{r}
faintr::compare_groups(
  fit    = ST_model, 
  higher = target_position == "right", 
  lower  = target_position == "left"
)
```

Although we did not hypothesize to find an effect, we see clear evidence for lower reaction times when the target appears on the right of the screen.

Finally, we also look at a potential main effect of SHAPE:

```{r}
faintr::compare_groups(
  fit    = ST_model, 
  higher = target_object == "circle", 
  lower  = target_object == "square"
)
```

The kind of object, circle or square, does not appear to have influenced reaction times.

In sum, we found the hypothesized (and well attested) effect of congruency, but we also found an unexpected effecrt of the target's position on the screen.
